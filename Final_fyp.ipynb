{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mappingproxy({'__module__': '__main__',\n",
       "              'errors': [Match({'ruleId': 'SENTENCE_FRAGMENT', 'message': '“When” at the beginning of a sentence usually requires a 2nd clause. Maybe a comma, question or exclamation mark is missing, or the sentence is incomplete and should be joined with the following sentence.', 'replacements': [], 'offsetInContext': 0, 'context': 'When I realized I cannot understand the worl...', 'offset': 0, 'errorLength': 4, 'category': 'GRAMMAR', 'ruleIssueType': 'grammar', 'sentence': 'When I realized I cannot understand the world.'}),\n",
       "               Match({'ruleId': 'COMMA_COMPOUND_SENTENCE', 'message': 'Use a comma before ‘and’ if it connects two independent clauses (unless they are closely connected and short).', 'replacements': ['side, and'], 'offsetInContext': 43, 'context': '... so happened that I was on the negative side and it was my job to convince the judges th...', 'offset': 416, 'errorLength': 8, 'category': 'PUNCTUATION', 'ruleIssueType': 'typographical', 'sentence': 'It so happened that I was on the negative side and it was my job to convince the judges that countries should continue manufacturing nuclear weapons.'})],\n",
       "              'sentiment': 'Negative',\n",
       "              'plagiarism': {'https://collegeessayguy.tumblr.com/post/101139595504/personal-statement-example-punk-rock': 27.730169193583826,\n",
       "               'https://www.quora.com/What-is-something-most-people-dont-understand': 6.390704429920116,\n",
       "               'https://www.coursehero.com/file/80437074/A2pdf/': 4.014869888475837,\n",
       "               'https://www.coursehero.com/file/82775212/course-hero-8docx/': 4.014869888475837,\n",
       "               'https://bccmalopolska.pl/great_debaters_answers.pdf': 1.6244805440120893,\n",
       "               'https://tinybuddha.com/blog/the-best-thing-to-say-to-someone-who-wont-understand-you/': 0.8837752209438052,\n",
       "               'https://psiloveyou.xyz/10-life-quotes-that-if-applied-will-change-the-way-you-see-the-world-forever-d05338ae489b': 0.6853547553508229,\n",
       "               'https://www.religiousforums.com/threads/i-just-realized-i-dont-understand-this-world.238156/': 0.23775741074510107,\n",
       "               'https://archive.globalpolicy.org/nations/future/2003/0530glob.htm': 0.1950743721043648},\n",
       "              'total_plagiarism': 45.77705570361179,\n",
       "              'Quality_of_Essay': 2.4,\n",
       "              'spell_error': 0.5,\n",
       "              'plagiarism_marks_lost': 1,\n",
       "              'sentiment_op': 'The expected attitude is considered to be in support of the topic but, Your approach is against and not relevenant\\t0',\n",
       "              'final_grade': 7,\n",
       "              'comment': 'Average',\n",
       "              'corrected_essay': \"When I realized I cannot understand the world. I recently debated at the Orange County Speech League Tournament, within the Parliamentary Division. This specific branch of debate is an hour long, and consists of two parties debating either side of a current political issue. In one particular debate, I was assigned the topic: “Should Nation States eliminate nuclear arms?” It so happened that I was on the negative side, and it was my job to convince the judges that countries should continue manufacturing nuclear weapons. During the debate, something strange happened: I realized that we are a special breed of species, that so much effort and resources are invested to ensure mutual destruction. And I felt that this debate in a small college classroom had elucidated something much more profound about the scale of human existence. In any case, I won 1st place at the tournament, but as the crowd cheered when my name was called to stand before an audience of hundreds of other debaters, and I flashed a victorious smile at the cameras, I couldn’t help but imagine that somewhere at that moment a nuclear bomb was being manufactured, adding to an ever-growing stockpile of doom. And that's when I realized that the world was something I will never understand.\",\n",
       "              '__dict__': <attribute '__dict__' of 'Myclass' objects>,\n",
       "              '__weakref__': <attribute '__weakref__' of 'Myclass' objects>,\n",
       "              '__doc__': None})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Disables warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Numerical Python for array manipulations.\n",
    "import numpy as np\n",
    "#Pandas for reading the dataset and converting it into dataframes.\n",
    "import pandas as pd\n",
    "#Math library for rounding off to next whole number.\n",
    "import math\n",
    "#os module to specify the directory path.\n",
    "import os\n",
    "import random\n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "#stopwords - words that do not add much meaning to the sentence.\n",
    "from nltk.corpus import stopwords\n",
    "#PorterStemmer - removes common morphological endings from words (tense, number, plural, etc.)\n",
    "from nltk.stem import PorterStemmer\n",
    "#SentimentIntensityAnalyzer - Implements and facilitates sentiment analysis tasks.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('vader_lexicon')     #Model for text sentiment analysis\n",
    "#nltk.download('punkt')             #Sentence tokenizer\n",
    "\n",
    "#CountVectorizer - Tokenizes collection of text and builds vocabulary of known words.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#TfidfVectorizer - Highlight interesting words.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Provides train/text split for training.\n",
    "from sklearn.model_selection import KFold\n",
    "#Used to fit a linear model.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#cohen-kappa-score is used to measure agreement between two raters.\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#Word2Vec creates word embeddings (Creates word vector for each word).\n",
    "from gensim.models import Word2Vec\n",
    "#KeyedVectors generates mapping between keys and vectors.\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Embedding translates high dimensional vectors and makes it easy to do ML on large inputs.\n",
    "from tensorflow.keras.layers import Embedding\n",
    "#pad-sequences ensures all sequences in a list have same length.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#A sequential model is used.\n",
    "from tensorflow.keras.models import Sequential\n",
    "#one-hot is used to one hot encode categorical values.\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "#LSTM layers for building the model.\n",
    "#Dropout layer to prevent overfitting.\n",
    "#Dense layers as the output layer.\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "#Lambda (Creates a nameless fuction for a short period of time).\n",
    "#Flatten layer is used to compress the input into 1D vector.\n",
    "from keras.layers import Lambda, Flatten\n",
    "#load_model is used to load a saved model.\n",
    "#model_from_config instantiates a keras model from its config.\n",
    "from keras.models import load_model, model_from_config\n",
    "#Keras backend API.\n",
    "import keras.backend as K\n",
    "from keras.models import model_from_json\n",
    "\n",
    "#Regular Expressions.\n",
    "import re, string\n",
    "\n",
    "#python spell checker.\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "#Python grammar checker.\n",
    "import language_tool_python\n",
    "\n",
    "#Allows to send HTTP requests.\n",
    "import requests\n",
    "#Helps to fetch data from XML and HTML files.\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#Used to compare a pair of inputs.\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "from sentiment import classifier\n",
    "\n",
    "\n",
    "def essay_wordlist(essay_1, rem_stopwords):\n",
    "    #match all strings without a letter and replace it with a white space character in the essay.\n",
    "    essay_1 = re.sub(\"[^A-Za-z]\", \" \", essay_1)\n",
    "    #Convert the essay into all lower case characters. \n",
    "    words = essay_1.lower().split()\n",
    "    #Removing stop words from the essay.\n",
    "    if rem_stopwords:\n",
    "        #creates a set of stopwords in english.\n",
    "        stop = set(stopwords.words(\"english\"))\n",
    "        #reassigns an essay containing no stop words.\n",
    "        words = [word1 for word1 in words if not word1 in stop]\n",
    "        \n",
    "    #return the words list.            \n",
    "    return (words)\n",
    "\n",
    "def essay_sentences(essay_1, rem_stopwords):\n",
    "    #Load the pre-trained punkt tokenizer for English.\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    #Tokenizing the essay.\n",
    "    sentence = tokenizer.tokenize(essay_1.strip())\n",
    "    sentences = []\n",
    "    #Generate word list for the tokenizer sentences.\n",
    "    for sentence1 in sentence:\n",
    "        if len(sentence1) > 0:\n",
    "            sentences.append(essay_wordlist(sentence1, rem_stopwords))\n",
    "    #Return the sentence list.\n",
    "    return sentences\n",
    "\n",
    "def FeatureVector(words, model, no_feat):\n",
    "    #Create an array filled with zeroes.\n",
    "    FeatureVector = np.zeros((no_feat,),dtype=\"float32\")\n",
    "    no_words = 0.\n",
    "    #Convert the list of names in the vocabulary into a set.\n",
    "    indextoword_set = set(model.wv.index2word)\n",
    "    #Calculate the word count and if a word is present in the vocabulary add it to the overall feature vector.\n",
    "    for x in words:\n",
    "        if x in indextoword_set:\n",
    "            no_words  = no_words + 1\n",
    "            FeatureVector = np.add(FeatureVector,model[x])\n",
    "    #Calculate the average.       \n",
    "    FeatureVector = np.divide(FeatureVector,no_words)\n",
    "    #Return the feature vector.\n",
    "    return FeatureVector\n",
    "\n",
    "def AvgFeatureVectors(essays, model, no_feat):\n",
    "    flag = 0\n",
    "    #Create another array with dimensions length of essay and number of features filled with zeroes.\n",
    "    FeatureVectors = np.zeros((len(essays),no_feat),dtype=\"float32\")\n",
    "    #For each essay append the average feature vector into the FeatureVector array.\n",
    "    for x in essays:\n",
    "        FeatureVectors[flag] = FeatureVector(x, model, no_feat)\n",
    "        flag = flag + 1\n",
    "    #Return the total average feature vector.\n",
    "    return FeatureVectors\n",
    "\n",
    "no_feat = 300 \n",
    "\n",
    "json_file = open('model1.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model1.h5\")\n",
    "\n",
    "content = \"When I realized I cannot understand the world. I recently debated at the Orange County Speech League Tournament, within the Parliamentary Division. This specific branch of debate is an hour long, and consists of two parties debating either side of a current political issue. In one particular debate, I was assigned the topic: “Should Nation States eliminate nuclear arms?” It so happened that I was on the negative side and it was my job to convince the judges that countries should continue manufacturing nuclear weapons. During the debate, something strange happened: I realized that we are a special breed of species, that so much effort and resources are invested to ensure mutual destruction. And I felt that this debate in a small college classroom had elucidated something much more profound about the scale of human existence. In any case, I won 1st place at the tournament, but as the crowd cheered when my name was called to stand before an audience of hundreds of other debaters, and I flashed a victorious smile at the cameras, I couldn’t help but imagine that somewhere at that moment a nuclear bomb was being manufactured, adding to an ever-growing stockpile of doom. And that's when I realized that the world was something I will never understand.\"\n",
    "\n",
    "#Load the saved word2vec model.\n",
    "model = KeyedVectors.load_word2vec_format( \"./word2vecmodel.bin\", binary=True)\n",
    "test_essays = []\n",
    "#Create a word list, average input features and reshape the input essay.\n",
    "test_essays.append(essay_wordlist( content, rem_stopwords=True ))\n",
    "test_vectors = AvgFeatureVectors( test_essays, model, no_feat )\n",
    "test_vectors = np.array(test_vectors)\n",
    "test_vectors = np.reshape(test_vectors, (test_vectors.shape[0], 1, test_vectors.shape[1]))\n",
    "\n",
    "#Generate grade prediction for the input essay.\n",
    "preds = loaded_model.predict(test_vectors)\n",
    "\n",
    "tool = language_tool_python.LanguageTool('en-US')\n",
    "\n",
    "#Parsing the input essay to detect syntactic errors.\n",
    "matches = tool.check(content)\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(content))\n",
    "\n",
    "sent1 = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "if sent1 == 'Positive':\n",
    "    sent = 1\n",
    "elif sent1 == 'Negative':\n",
    "    sent = 0\n",
    "    \n",
    "def search(query, num):\n",
    "    \n",
    "    #Define a URL to perform searching\n",
    "    url = 'https://www.bing.com/search?q=' + query\n",
    "    url1 = []\n",
    "    \n",
    "    #Generae a HTTP request to the URL\n",
    "    x = requests.get(url, headers = {'User-agent': 'John Doe'})\n",
    "    #Fetch the data in the site using beautifulsoup.\n",
    "    y = bs(x.text, 'html.parser')\n",
    "    \n",
    "    #Append the current examined URL into a list.\n",
    "    for a in y.find_all('a'):\n",
    "        url = str(a.get('href'))\n",
    "        if url.startswith('http'):\n",
    "            if not url.startswith('http://go.m') and not url.startswith('https://go.m'):\n",
    "                url1.append(url)\n",
    "    \n",
    "    return url1[:num]\n",
    "\n",
    "def extract(url):\n",
    "    x = requests.get(url)\n",
    "    y = bs(x.text, 'html.parser')\n",
    "    #Return the text from the site.\n",
    "    return y.get_text()\n",
    "#Define Stopping words.\n",
    "stopping_words = set(nltk.corpus.stopwords.words('english')) \n",
    "\n",
    "def TokenizeText(string):\n",
    "    #Tokenize the string in the site.\n",
    "    words = nltk.word_tokenize(string)\n",
    "    #Return all the non stopping words.\n",
    "    return (\" \".join([word for word in words if word not in stopping_words]))\n",
    "\n",
    "def Verify(string, results_per_sentence):\n",
    "    #Sentence tokenize the string in the site.\n",
    "    sentences = nltk.sent_tokenize(string)\n",
    "    matching_sites = []\n",
    "    #Detect URLs where similar content is found.\n",
    "    for url in search(query=string, num=results_per_sentence):\n",
    "        matching_sites.append(url)\n",
    "    #Detct the sentences in the URL.\n",
    "    for sentence in sentences:\n",
    "        for url in search(query = sentence, num = results_per_sentence):\n",
    "            matching_sites.append(url)\n",
    "    \n",
    "    #Return the URLs\n",
    "    return (list(set(matching_sites)))\n",
    "\n",
    "def similarity(str1, str2):\n",
    "    #Match the entire (100%) two contents and return the ratio of similarities between the two texts.\n",
    "    return (SequenceMatcher(None,str1,str2).ratio())*100\n",
    "\n",
    "\n",
    "def result(text):\n",
    "    \n",
    "    #Copare the two texts.\n",
    "    matching_sites = Verify(TokenizeText(text), 2)\n",
    "    matches = {}\n",
    "\n",
    "    #For each matching site determine the amount of similarity.\n",
    "    for i in range(len(matching_sites)):\n",
    "        matches[matching_sites[i]] = similarity(text, extract(matching_sites[i]))\n",
    "\n",
    "    #Sort the similarities in descending order\n",
    "    matches = {k: v for k, v in sorted(matches.items(), key=lambda item: item[1], reverse=True)}\n",
    "\n",
    "    #Return the URLs and their corresponding plagiarized percentage score as a dictionary.\n",
    "    return matches\n",
    "\n",
    "plag = {}\n",
    "total = 0\n",
    "plag = result(content)\n",
    "#For each key, value pair in the dictionary, display the URL and the amount of % plagiarism.\n",
    "for key, value in plag.items():\n",
    "    #Calculate the total plagiarism percentage.\n",
    "    total = total+value\n",
    "\n",
    "expected_sent = 1\n",
    "x = math.ceil(preds/2)\n",
    "a = (x * 3)/10\n",
    "b = 0.25 * len(matches)\n",
    "if b > 3:\n",
    "    b = 3\n",
    "if total <= 15:\n",
    "    c = 0\n",
    "elif total > 15 and total <= 15:\n",
    "    c = 0.5\n",
    "elif total > 25 and total <= 50:\n",
    "    c = 1\n",
    "elif total > 50 and total <= 75:\n",
    "    c = 1.5\n",
    "elif total > 75 and total <= 85:\n",
    "    c = 2\n",
    "elif total > 85 and total <= 95:\n",
    "    c = 2.5\n",
    "elif total > 95:\n",
    "    c = 3\n",
    "    \n",
    "if sent == expected_sent:\n",
    "    d = 0\n",
    "else:\n",
    "    d = 1\n",
    "if expected_sent == 0.5:\n",
    "    d = 0\n",
    "    \n",
    "sent_op = \"\"\n",
    "\n",
    "a1 = np.around(a, decimals = 2)\n",
    "if expected_sent == sent:\n",
    "    sent_op = \"Your approach towards the topic is acceptable and relevant\\t1\"\n",
    "elif expected_sent == 1 and sent == 0:\n",
    "    sent_op=\"The expected attitude is considered to be in support of the topic but, Your approach is against and not relevenant\\t0\"\n",
    "elif expected_sent == 0 and sent == 1:\n",
    "    sent_op = \"The expected attitude is considered not to be in support of the topic but, Your approach is supporting and not relevenant\\t0\"\n",
    "    \n",
    "score = 10 - (3-a) - b - c - d\n",
    "\n",
    "final_score = math.ceil(score)\n",
    "\n",
    "if final_score < 5:\n",
    "    y = \"Poor\"\n",
    "#Else if the predicted score is between 5 and 8 the grade is average.\n",
    "elif final_score >= 5 and final_score < 8:\n",
    "    y = \"Average\"\n",
    "#If the predicted score is greater than or equal to 8 then the grade is Excellent.\n",
    "else:\n",
    "    y = \"Excellent\"\n",
    "\n",
    "mistakes = []\n",
    "corrections = []\n",
    "positions1 = []\n",
    "positions2 = []\n",
    "\n",
    "#For each syntactical mistake in the essay, replace the mistake with the appropriate correction at the corresponding offset value and the error length in a new list.\n",
    "for a in matches:\n",
    "    if len(a.replacements)>0:\n",
    "        positions1.append(a.offset)\n",
    "        positions2.append(a.errorLength+a.offset)\n",
    "        mistakes.append(content[a.offset:a.errorLength+a.offset])\n",
    "        corrections.append(a.replacements[0])\n",
    "     \n",
    " \n",
    "#Create a list of the input essay.\n",
    "new_text = list(content)\n",
    "\n",
    "#Create a new string of text based on the values in the mistakes list and the original list by joining the two lists appropriately.\n",
    "for m in range(len(positions1)):\n",
    "    for i in range(len(content)):\n",
    "        new_text[positions1[m]] = corrections[m]\n",
    "        if (i>positions1[m] and i<positions2[m]):\n",
    "            new_text[i]=\"\"\n",
    "     \n",
    "new_text = \"\".join(new_text)\n",
    "new_text\n",
    "\n",
    "class Myclass:\n",
    "    errors = matches\n",
    "    sentiment = sent1\n",
    "    plagiarism = plag\n",
    "    total_plagiarism = total\n",
    "    Quality_of_Essay = a1\n",
    "    spell_error = b\n",
    "    plagiarism_marks_lost= c\n",
    "    sentiment_op = sent_op\n",
    "    final_grade = final_score\n",
    "    comment = y\n",
    "    corrected_essay = new_text\n",
    "\n",
    "temp = vars(Myclass)\n",
    "\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
