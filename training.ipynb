{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/guru/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/guru/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/guru/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/guru/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/guru/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/guru/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "#Numerical Python for array manipulations.\n",
    "import numpy as np\n",
    "#Pandas for reading the dataset and converting it into dataframes.\n",
    "import pandas as pd\n",
    "#Math library for rounding off to next whole number.\n",
    "import math\n",
    "#os module to specify the directory path.\n",
    "import os\n",
    "import random\n",
    "\n",
    "from keras.layers import Bidirectional\n",
    " \n",
    "\n",
    "\n",
    "#Disables warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Natural Language Toolkit\n",
    "import nltk\n",
    "#stopwords - words that do not add much meaning to the sentence.\n",
    "from nltk.corpus import stopwords\n",
    "#PorterStemmer - removes common morphological endings from words (tense, number, plural, etc.)\n",
    "from nltk.stem import PorterStemmer\n",
    "#SentimentIntensityAnalyzer - Implements and facilitates sentiment analysis tasks.\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import FreqDist\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('vader_lexicon')     #Model for text sentiment analysis\n",
    "#nltk.download('punkt')             #Sentence tokenizer\n",
    "\n",
    "#CountVectorizer - Tokenizes collection of text and builds vocabulary of known words.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#TfidfVectorizer - Highlight interesting words.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#Provides train/text split for training.\n",
    "from sklearn.model_selection import KFold\n",
    "#Used to fit a linear model.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "#cohen-kappa-score is used to measure agreement between two raters.\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "#Word2Vec creates word embeddings (Creates word vector for each word).\n",
    "from gensim.models import Word2Vec\n",
    "#KeyedVectors generates mapping between keys and vectors.\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "#Embedding translates high dimensional vectors and makes it easy to do ML on large inputs.\n",
    "from tensorflow.keras.layers import Embedding\n",
    "#pad-sequences ensures all sequences in a list have same length.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "#A sequential model is used.\n",
    "from tensorflow.keras.models import Sequential\n",
    "#one-hot is used to one hot encode categorical values.\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "#LSTM layers for building the model.\n",
    "#Dropout layer to prevent overfitting.\n",
    "#Dense layers as the output layer.\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "#Lambda (Creates a nameless fuction for a short period of time).\n",
    "#Flatten layer is used to compress the input into 1D vector.\n",
    "from keras.layers import Lambda, Flatten\n",
    "#load_model is used to load a saved model.\n",
    "#model_from_config instantiates a keras model from its config.\n",
    "from keras.models import load_model, model_from_config\n",
    "#Keras backend API.\n",
    "import keras.backend as K\n",
    "\n",
    "#Regular Expressions.\n",
    "import re, string\n",
    "\n",
    "#python spell checker.\n",
    "#from spellchecker import SpellChecker\n",
    "\n",
    "#Python grammar checker.\n",
    "import language_tool_python\n",
    "\n",
    "#Allows to send HTTP requests.\n",
    "import requests\n",
    "#Helps to fetch data from XML and HTML files.\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#Used to compare a pair of inputs.\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./fyp/training_set_rel3.tsv\", sep='\\t', encoding='ISO-8859-1')\n",
    "\n",
    "def define_model():\n",
    "    #Declare a sequential model.\n",
    "    model = Sequential()\n",
    "    #Add two LSTM layers a dropout layer and a dense layer with rectified linear unit as the activation function and a single output unit.\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.3))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    #Compile the model\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['accuracy','mae'])\n",
    "    #model.summary()\n",
    "    \n",
    "    #Return the defined model.\n",
    "    return model\n",
    "X=df\n",
    "y = df['domain1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_wordlist(essay_1, rem_stopwords):\n",
    "    #match all strings without a letter and replace it with a white space character in the essay.\n",
    "    essay_1 = re.sub(\"[^A-Za-z]\", \" \", essay_1)\n",
    "    #Convert the essay into all lower case characters. \n",
    "    words = essay_1.lower().split()\n",
    "    #Removing stop words from the essay.\n",
    "    if rem_stopwords:\n",
    "        #creates a set of stopwords in english.\n",
    "        stop = set(stopwords.words(\"english\"))\n",
    "        #reassigns an essay containing no stop words.\n",
    "        words = [word1 for word1 in words if not word1 in stop]\n",
    "        \n",
    "    #return the words list.            \n",
    "    return (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def essay_sentences(essay_1, rem_stopwords):\n",
    "    #Load the pre-trained punkt tokenizer for English.\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    #Tokenizing the essay.\n",
    "    sentence = tokenizer.tokenize(essay_1.strip())\n",
    "    sentences = []\n",
    "    #Generate word list for the tokenizer sentences.\n",
    "    for sentence1 in sentence:\n",
    "        if len(sentence1) > 0:\n",
    "            sentences.append(essay_wordlist(sentence1, rem_stopwords))\n",
    "    #Return the sentence list.\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureVector(words, model, no_feat):\n",
    "    #Create an array filled with zeroes.\n",
    "    FeatureVector = np.zeros((no_feat,),dtype=\"float32\")\n",
    "    no_words = 0.\n",
    "    #Convert the list of names in the vocabulary into a set.\n",
    "    indextoword_set = set(model.wv.index2word)\n",
    "    #Calculate the word count and if a word is present in the vocabulary add it to the overall feature vector.\n",
    "    for x in words:\n",
    "        if x in indextoword_set:\n",
    "            no_words  = no_words + 1\n",
    "            FeatureVector = np.add(FeatureVector,model[x])\n",
    "    #Calculate the average.       \n",
    "    FeatureVector = np.divide(FeatureVector,no_words)\n",
    "    #Return the feature vector.\n",
    "    return FeatureVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AvgFeatureVectors(essays, model, no_feat):\n",
    "    flag = 0\n",
    "    #Create another array with dimensions length of essay and number of features filled with zeroes.\n",
    "    FeatureVectors = np.zeros((len(essays),no_feat),dtype=\"float32\")\n",
    "    #For each essay append the average feature vector into the FeatureVector array.\n",
    "    for x in essays:\n",
    "        FeatureVectors[flag] = FeatureVector(x, model, no_feat)\n",
    "        flag = flag + 1\n",
    "    #Return the total average feature vector.\n",
    "    return FeatureVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "#Define 5 splits for KFOLD training.\n",
    "x = KFold(n_splits = 5, shuffle = True)\n",
    "output = []\n",
    "y_pred1 = []\n",
    "\n",
    "fold = 1\n",
    "#Perform training by creating a list from the dataset for each train and test datasets for 5 folds.\n",
    "for train, test in x.split(X):\n",
    "    #print(\"\\nFold {}\\n\".format(fold))\n",
    "    #Declare test and train sets for each fold.\n",
    "    x_train, x_test, y_train, y_test = X.iloc[train], X.iloc[test], y.iloc[train], y.iloc[test]\n",
    "    \n",
    "    #Define the test and train essays from the 'essay' column of the dataset.\n",
    "    training_essays = x_train['essay']\n",
    "    testing_essays = x_test['essay']\n",
    "    \n",
    "    a = []\n",
    "    \n",
    "    #Sentence tokenize each training essay.\n",
    "    for essay in training_essays:\n",
    "            a = a + essay_sentences(essay, rem_stopwords = True)\n",
    "            \n",
    "    no_feat = 300 \n",
    "    word_count = 40\n",
    "    no_workers = 4\n",
    "    cont = 10\n",
    "    sample = 1e-3\n",
    "\n",
    "    #Predict the nearby words for each word in the sentence.\n",
    "    model = Word2Vec(a, workers=no_workers, size=no_feat, min_count = word_count, window = cont, sample = sample)\n",
    "\n",
    "    #Normalize vectors (Equal length)\n",
    "    model.init_sims(replace=True)\n",
    "    #Save the model.\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    cleaning_train_essays = []\n",
    "    \n",
    "    #For each training essay generate a word list.\n",
    "    for essay_1 in training_essays:\n",
    "        cleaning_train_essays.append(essay_wordlist(essay_1, rem_stopwords=True))\n",
    "    #Generate average feature vectors for the word lists.\n",
    "    Vectors_train = AvgFeatureVectors(cleaning_train_essays, model, no_feat)\n",
    "    \n",
    "    #Similarly for the test essays generate word lists and average feature vectors.\n",
    "    cleaning_test_essays = []\n",
    "    for essay_1 in testing_essays:\n",
    "        cleaning_test_essays.append(essay_wordlist( essay_1, rem_stopwords=True ))\n",
    "    Vectors_test = AvgFeatureVectors( cleaning_test_essays, model, no_feat )\n",
    "    \n",
    "    #Reshape the average feature vectors of test and train datasets to the shape of first dimension of the respective data vectors.\n",
    "    Vectors_train = np.array(Vectors_train)\n",
    "    Vectors_test = np.array(Vectors_test)\n",
    "    Vectors_train = np.reshape(Vectors_train, (Vectors_train.shape[0], 1, Vectors_train.shape[1]))\n",
    "    Vectors_test = np.reshape(Vectors_test, (Vectors_test.shape[0], 1, Vectors_test.shape[1]))\n",
    "    \n",
    "    #Assign the defined model.\n",
    "    lstm_model = define_model()\n",
    "    #Fit the model.\n",
    "    lstm_model.fit(Vectors_train, y_train, batch_size=64, epochs=10, verbose = 0)\n",
    "    #Load the model weights.\n",
    "    #lstm_model.load_weights('./fyp/model.h5')\n",
    "    y_predict = lstm_model.predict(Vectors_test)\n",
    "    \n",
    "    #Save the model when all the folds are completed.\n",
    "    if fold == 5:\n",
    "         #lstm_model.save('./fyp/model.h5')\n",
    "        model_json = lstm_model.to_json()\n",
    "        with open(\"model1.json\", \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "        lstm_model.save_weights(\"model1.h5\")\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "    \n",
    "    #Round off the predicted value.\n",
    "    y_predict = np.around(y_predict)\n",
    "    \n",
    "    #Generate a kappa score for each fold.\n",
    "    result = cohen_kappa_score(y_test.values,y_predict,weights='quadratic')\n",
    "    #print(\"Kappa Score for fold {fold} is {score}\".format(fold = fold, score = result))\n",
    "    #Add each kappa score to the overall score.\n",
    "    output.append(result)\n",
    "\n",
    "    #Increment the value of fold.\n",
    "    fold = fold + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
